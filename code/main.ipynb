{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 安装依赖的第三方库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data process\n",
    "从原始70个特征中选择出39个相关特征，并保存为半精度的pt文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Desktop\\weather_code\\code\n"
     ]
    }
   ],
   "source": [
    "# 将路径加入系统变量\n",
    "import sys\n",
    "import os\n",
    "path = os.path.abspath('.').split('\\\\')[0:-1]\n",
    "path = '\\\\'.join(path)\n",
    "sys.path.append(path)\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "print(os.path.abspath('.'))\n",
    "# print(sys.path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T20:57:13.038839Z",
     "end_time": "2023-09-15T20:57:13.073080Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "channels = xr.open_zarr('../data/weather_round1_train_2007').x.channel.values\n",
    "\n",
    "# 通过均值和方差，找出的与目标特征相关的特征通道\n",
    "target_features = {'t2m': ['z150', 'z200', 'z250', 'z300', 'z400', 'z500', 't100', 't150', 't250',\n",
    "                           't300', 't400', 't500', 't600', 't700', 'u50', 'r150', 't2m'],   # >0.9\n",
    "                   'u10': ['t100', 't150', 't700', 'u50', 'u850', 'u925', 'u1000', 'r50', 'r100', 't2m', 'u10'],    # >0.75\n",
    "                   'v10': ['v1000', 'v10', 'v925', 'u1000', 'u10', 'u925'],   # 均值相关的只有三个，加上了方差相关的\n",
    "                   'msl': ['z1000', 'u50', 'u100', 'u150', 'u200', 'u250', 'u300', 'u400', 'r50',\n",
    "                           'r100', 'r150', 'r200', 'msl', 'tp'],    # >0.8\n",
    "                   'tp': ['t100', 't150', 't250', 't300', 't400', 't500', 'u50', 'r50', 'r100',\n",
    "                          'r150', 'r200', 'r850', 'r925', 'r1000', 'msl', 'tp'] # >0.8\n",
    "                   }\n",
    "\n",
    "# 把字典的所有值合并成一个没有重复元素的集合\n",
    "all_features = list(set(sum(target_features.values(), [])))\n",
    "all_features = [feature for feature in channels if feature in all_features]\n",
    "print(len(all_features), '\\n', all_features)\n",
    "\n",
    "\n",
    "def load_dataset(s_year, e_year):\n",
    "    ds = []\n",
    "    for y in range(s_year, e_year+1):\n",
    "        data_name = os.path.join(f'../data/weather_round1_train_{y}')\n",
    "        x = xr.open_zarr(data_name, consolidated=True)\n",
    "        print(f'{data_name}, {x.time.values[0]} ~ {x.time.values[-1]}')\n",
    "        ds.append(x)\n",
    "    ds = xr.concat(ds, 'time')\n",
    "    return ds\n",
    "\n",
    "for year in range(2007, 2012):\n",
    "    data = load_dataset(year,year).x\n",
    "    data = data.sel(channel=all_features).values.astype(np.float16)\n",
    "    data = torch.from_numpy(data)\n",
    "    torch.save(data, f'../data/{year}_39.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练部分（Training\n",
    "修改SimVP模型，使其预测39*2个特征，输出5*2个目标变量。其中2是时间步。由于需要预测未来20步的数据，这里通过多次不同的预测实验，选择使用10个相同架构的模型，独立预测。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timm.scheduler import create_scheduler\n",
    "from utils.tools import getModelSize, load_model, save_model, EMA\n",
    "from utils.eval import fourcastnet_pretrain_evaluate\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "from model.SimVP.model import SimVP as Model\n",
    "\n",
    "SAVE_PATH = Path('./checkpoint/')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "\"\"\" 准备训练集和验证集 \"\"\"\n",
    "channels = xr.open_zarr('../data/weather_round1_train_2007').x.channel.values\n",
    "# 通过均值和方差，找出的与目标特征相关的特征通道\n",
    "target_features = {'t2m': ['z150', 'z200', 'z250', 'z300', 'z400', 'z500', 't100', 't150', 't250',\n",
    "                           't300', 't400', 't500', 't600', 't700', 'u50', 'r150', 't2m'],   # >0.9\n",
    "                   'u10': ['t100', 't150', 't700', 'u50', 'u850', 'u925', 'u1000', 'r50', 'r100', 't2m', 'u10'],    # >0.75\n",
    "                   'v10': ['v1000', 'v10', 'v925', 'u1000', 'u10', 'u925'],   # 均值相关的只有三个，加上了方差相关的\n",
    "                   'msl': ['z1000', 'u50', 'u100', 'u150', 'u200', 'u250', 'u300', 'u400', 'r50',\n",
    "                           'r100', 'r150', 'r200', 'msl', 'tp'],    # >0.8\n",
    "                   'tp': ['t100', 't150', 't250', 't300', 't400', 't500', 'u50', 'r50', 'r100',\n",
    "                          'r150', 'r200', 'r850', 'r925', 'r1000', 'msl', 'tp'] # >0.8\n",
    "                   }\n",
    "\n",
    "# 把字典的所有值合并成一个没有重复元素的集合\n",
    "all_features = list(set(sum(target_features.values(), [])))\n",
    "all_features = [feature for feature in channels if feature in all_features]\n",
    "print(len(all_features), '\\n', all_features)\n",
    "\n",
    "\n",
    "class ERA5(Dataset):\n",
    "    def __init__(self, data, pred_s, pred_e):\n",
    "        self.data = data\n",
    "        self.pred_s = pred_s\n",
    "        self.pred_e = pred_e\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - 2 - 20 + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+2] # [B, t, c, h, w] == [B, 2, 70, h, w]\n",
    "        y = self.data[idx+2+self.pred_s:idx+2+self.pred_e, -5:]   # [B,C,H,W] == [20, 5, h, w]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_data = torch.load('../data/2007_39.pt')\n",
    "print(\"Load training data...\\n\", train_data.shape)\n",
    "for year in range(2008, 2011):\n",
    "    data = torch.load(f'../data/{year}_39.pt')\n",
    "    train_data = torch.cat((train_data, data), dim=0)\n",
    "    print(train_data.shape)\n",
    "\n",
    "val_data = torch.load(f'../data/2011_39.pt')\n",
    "print('\\nLoad validation data... \\n', val_data.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pretrain_one_epoch(epoch, start_step, model, criterion, data_loader, optimizer, loss_scaler, lr_scheduler, min_loss, writer=None, ema=None):\n",
    "    loss_val = torch.tensor(0., device=\"cuda\")\n",
    "    count = torch.tensor(1e-5, device=\"cuda\")\n",
    "\n",
    "    model.train()\n",
    "    global_step = epoch*len(data_loader)\n",
    "    threshold = 0.001\n",
    "\n",
    "    for step, batch in enumerate(tqdm(data_loader, leave=True, position=0)):\n",
    "        if step < start_step:\n",
    "            continue\n",
    "\n",
    "        x, y = [x.cuda(non_blocking=True) for x in batch]\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            if torch.isnan(loss).int().sum() == 0:\n",
    "                count += 1\n",
    "                loss_val += loss\n",
    "\n",
    "        loss_scaler.scale(loss).backward()\n",
    "\n",
    "        # # 记录每一层的梯度信息\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         nan_count = torch.isnan(param.grad).sum().item()\n",
    "        #         below_threshold_count = (param.grad.abs() < threshold).sum().item()\n",
    "        #         # writer.add_scalar(f'Gradient/{name}/NaN Count', nan_count, global_step+step)\n",
    "        #         writer.add_scalar(f'Gradient/{name}/Below Threshold Count', below_threshold_count, global_step+step)\n",
    "\n",
    "\n",
    "        loss_scaler.step(optimizer)\n",
    "        loss_scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        ema.update()\n",
    "\n",
    "    return loss_val.item() / count.item()\n",
    "\n",
    "\n",
    "def train(args, pred_s, pred_e):\n",
    "\n",
    "    data_train, data_val = ERA5(train_data, pred_s, pred_e), ERA5(val_data, pred_s, pred_e)\n",
    "\n",
    "    model = Model().cuda()\n",
    "    ema = EMA(model, 0.999)\n",
    "    ema.register()\n",
    "    param_sum, buffer_sum, all_size = getModelSize(model)\n",
    "    print(f\"Number of Parameters: {param_sum}, Number of Buffers: {buffer_sum}, Size of Model: {all_size:.4f} MB\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.95))\n",
    "    loss_scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # writer = SummaryWriter(f'runs/model')\n",
    "    # input = torch.randn((1, 2, 39, 161, 161)).cuda()\n",
    "    # writer.add_graph(model, (input,))\n",
    "    # writer.close()\n",
    "\n",
    "    # load data\n",
    "    train_dataloader = DataLoader(data_train, args.batch_size, num_workers=0, shuffle=True, pin_memory=True, drop_last=False)  # , num_workers=8, pin_memory=True\n",
    "    val_dataloader = DataLoader(data_val, args.batch_size, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # load\n",
    "    start_epoch, start_step, min_loss, early_stop = load_model(model, ema, optimizer, lr_scheduler, loss_scaler, SAVE_PATH / f'backbone_best_{pred_s}_{pred_e}.pt')  # backbone_best\n",
    "    print(f\"Start pretrain for {args.pretrain_epochs} epochs, now {start_epoch}/{args.pretrain_epochs}\")\n",
    "    print(f'min_loss:{min_loss}')\n",
    "\n",
    "    writer = None\n",
    "    last_loss = min_loss\n",
    "    early_stop = 0\n",
    "\n",
    "    for epoch in range(start_epoch, args.pretrain_epochs):\n",
    "        # 创建TensorBoard记录器\n",
    "        # writer = SummaryWriter(f'runs/epoch{epoch}')\n",
    "\n",
    "        t0 = time.time()\n",
    "        train_loss = pretrain_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler, lr_scheduler, min_loss, writer, ema)  #\n",
    "        t1 = time.time()\n",
    "        # writer.close()\n",
    "        start_step = 0\n",
    "        lr_scheduler.step(epoch)\n",
    "\n",
    "        val_loss = fourcastnet_pretrain_evaluate(val_dataloader, model, criterion, ema)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}, Time: {t1-t0:.2f}s, Early stop: {early_stop}/3\")\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            save_model(model, epoch + 1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, early_stop, ema, SAVE_PATH / f'backbone_best_{pred_s}_{pred_e}.pt')\n",
    "        else:\n",
    "            if last_loss < val_loss:\n",
    "                early_stop += 1\n",
    "            else:\n",
    "                early_stop = 0\n",
    "            last_loss = val_loss\n",
    "            if early_stop >= 3:\n",
    "                print(f'Early Stop at Epoch {epoch}')\n",
    "                save_model(model, epoch + 1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, early_stop, ema, SAVE_PATH / f'backbone_best_{pred_s}_{pred_e}.pt')\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        raise AttributeError(f\"'AttrDict' object has no attribute '{name}'\")\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "def get_args():\n",
    "    args = dict(batch_size=44, pretrain_epochs=30, fintune_epochs=25, drop=0.0, drop_path=0.1,\n",
    "                opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1, momentum=0.9, weight_decay=0.05, sched='cosine',\n",
    "                lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05,\n",
    "                decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1,\n",
    "                color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic',\n",
    "                repeated_aug=False, reprob=0, remode='pixel', recount=1, resplit=False, fno_bias=False, fno_blocks=4,\n",
    "                fno_softshrink=0.0, double_skip=False, tensorboard_dir=None, hidden_size=256, num_layers=12,\n",
    "                checkpoint_activations=False, autoresume=False, num_attention_heads=1, ls_w=4, ls_dp_rank=16)\n",
    "    args = AttrDict(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "for pred_s, pred_e in zip(range(0,22,2), range(2,22,2)):\n",
    "    print(f'\\npred_s: {pred_s}, pred_e: {pred_e}')\n",
    "    t_start = time.time()\n",
    "    train(args, pred_s, pred_e)\n",
    "    t_end = time.time()\n",
    "    print(f'Train model || pred_s: {pred_s}, pred_e: {pred_e}, total_time: {t_end - t_start}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 预测部分（Inference\n",
    "使用保存的10个模型，分别预测对应的部分，然后拼接起来，得到最终的预测结果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target_features = {'t2m': ['z150', 'z200', 'z250', 'z300', 'z400', 'z500', 't100', 't150', 't250',\n",
    "                           't300', 't400', 't500', 't600', 't700', 'u50', 'r150', 't2m'],   # >0.9\n",
    "                   'u10': ['t100', 't150', 't700', 'u50', 'u850', 'u925', 'u1000', 'r50', 'r100', 't2m', 'u10'],    # >0.75\n",
    "                   'v10': ['v1000', 'v10', 'v925', 'u1000', 'u10', 'u925'],   # 均值相关的只有三个，加上了方差相关的\n",
    "                   'msl': ['z1000', 'u50', 'u100', 'u150', 'u200', 'u250', 'u300', 'u400', 'r50',\n",
    "                           'r100', 'r150', 'r200', 'msl', 'tp'],    # >0.8\n",
    "                   'tp': ['t100', 't150', 't250', 't300', 't400', 't500', 'u50', 'r50', 'r100',\n",
    "                          'r150', 'r200', 'r850', 'r925', 'r1000', 'msl', 'tp'] # >0.8\n",
    "                   }\n",
    "\n",
    "# 把字典的所有值合并成一个没有重复元素的集合\n",
    "all_features = list(set(sum(target_features.values(), [])))\n",
    "all_features = [feature for feature in channels if feature in all_features]\n",
    "print(len(all_features), '\\n', all_features)\n",
    "feature_idx = []\n",
    "for feature in all_features:\n",
    "    feature_idx.append(int(np.where(np.isin(channels, feature))[0][0]))\n",
    "print(feature_idx)\n",
    "# target_features = {key: [item for item in channels if item in value] for key, value in target_features.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 把数据一次全部读取，放入dataset和dataloader\n",
    "data = []\n",
    "for idx in range(300):\n",
    "    idx = str(idx).zfill(3)     # 将idx转换成3位数的字符串，在前面补0\n",
    "    data.append(torch.load(f'../{idx}.pt', map_location='cpu'))\n",
    "\n",
    "\n",
    "model_list = []\n",
    "for s,e in zip(range(0,20,2),range(2,22,2)):\n",
    "    model = Model((2, 39, 161, 161)).cuda()\n",
    "    model.load_state_dict(torch.load(f'backbone_best_{s}_{e}.pt')['ema']['shadow'])\n",
    "    model_list.append(model)\n",
    "\n",
    "\n",
    "if not os.path.exists('../submit'):\n",
    "    os.mkdir('../submit')\n",
    "\n",
    "if not os.path.exists('../submit/output'):\n",
    "    os.mkdir('../submit/output')\n",
    "\n",
    "\n",
    "# inference\n",
    "with torch.no_grad():\n",
    "    for i in range(300):\n",
    "        idx = str(i).zfill(3)\n",
    "        data_in = data[i].float().unsqueeze(0).cuda()\n",
    "        data_in = data_in[:, :, feature_idx]\n",
    "        output = []\n",
    "        for model in model_list:\n",
    "            output.append(model(data_in))\n",
    "        output = torch.cat(output, dim=1).squeeze(0).half()\n",
    "        torch.save(output, f'../submit/output/{idx}.pt')\n",
    "        print(f'save {idx}.pt ...')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 将预测结果转成压缩文件"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "path = '../submit/output/'  # 要压缩的文件路径\n",
    "zipName = '../submit/output.zip'  # 压缩后的zip文件路径及名称\n",
    "\n",
    "# 创建一个新的zip文件\n",
    "f = zipfile.ZipFile(zipName, 'w', zipfile.ZIP_DEFLATED)\n",
    "#使用zipfile模块创建一个新的zip文件对象，指定为写模式('w')并采用ZIP_DEFLATED压缩算法。\n",
    "\n",
    "# 遍历指定路径下的所有文件和文件夹\n",
    "for dirpath, dirnames, filenames in os.walk(path): #使用os.walk函数遍历指定路径下的所有文件和文件夹，包括子文件夹\n",
    "    for filename in filenames: #遍历每个文件夹中的文件名。\n",
    "        print(filename)\n",
    "        # 将文件添加到zip文件中\n",
    "        f.write(os.path.join(dirpath, filename))\n",
    "\n",
    "# 关闭zip文件\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-15T21:31:39.535616Z",
     "end_time": "2023-09-15T21:31:39.581461Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
