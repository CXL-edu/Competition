{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-06T00:30:12.396355600Z",
     "start_time": "2023-09-06T00:30:12.393370400Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:\\weather\\Lib\\site-packages')\n",
    "# 打印环境变量\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [19.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "# model = nn.Conv2d(1,1,5,2,2)\n",
    "# x = torch.randn(1, 1, 160, 160)\n",
    "# print(model(x).shape)\n",
    "# # print(F.interpolate(x,[81,81], mode='bicubic').shape)\n",
    "\n",
    "# x = torch.randn((2,1,10,10)).unsqueeze(0).repeat(20, 1, 1, 1, 1)\n",
    "# # x = x.repeat(20,1,1,1)\n",
    "# print(x.shape)\n",
    "# print(x[1,:,0,0,:])\n",
    "# print(x[8,:,0,0,:])\n",
    "times = torch.arange(0, 20, dtype=torch.float32).unsqueeze(-1)\n",
    "print(times)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T03:32:14.245301Z",
     "start_time": "2023-09-06T03:32:14.241784800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([20, 25921])\n",
      "torch.Size([1, 25921])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, padding_mode='replicate' dilation=1, groups=1, bias=True)\n",
    "# model = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "# model = nn.Conv2d(1, 1, 3, stride=2, padding=1)\n",
    "model = nn.Embedding(20,161*161)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "x = torch.tensor([1])\n",
    "print(model(x).shape)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.05, betas=(0.9, 0.95))\n",
    "#\n",
    "# x = torch.randn(1,1,80,80)\n",
    "# y = torch.randn(1,1,160,160)\n",
    "# preds = model(x)\n",
    "#\n",
    "# loss = criterion(preds, y)\n",
    "# loss.backward()\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.grad)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T03:00:33.829770Z",
     "start_time": "2023-09-06T03:00:33.823241200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# import xarray as xr\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timm.scheduler import create_scheduler\n",
    "from utils.tools import getModelSize, load_model, save_model\n",
    "from utils.eval import climax_pretrain_evaluate\n",
    "\n",
    "\n",
    "SAVE_PATH = Path('./checkpoint/')\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from climax.regional_forecast.arch import RegionalClimaX\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T14:12:47.578219600Z",
     "start_time": "2023-09-05T14:12:46.161091300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1260, 39, 161, 161]) torch.float32\n",
      "torch.Size([200, 39, 161, 161]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_features = {'t2m': ['z150', 'z200', 'z250', 'z300', 'z400', 'z500', 't100', 't150', 't250',\n",
    "                           't300', 't400', 't500', 't600', 't700', 'u50', 'r150', 't2m'],  # >0.9\n",
    "                   'u10': ['t100', 't150', 't700', 'u50', 'u850', 'u925', 'u1000', 'r50', 'r100', 't2m', 'u10'],\n",
    "                   # >0.75\n",
    "                   'v10': ['v1000', 'v10', 'v925', 'u1000', 'u10', 'u925'],  # 均值相关的只有三个，加上了方差相关的\n",
    "                   'msl': ['z1000', 'u50', 'u100', 'u150', 'u200', 'u250', 'u300', 'u400', 'r50',\n",
    "                           'r100', 'r150', 'r200', 'msl', 'tp'],  # >0.8\n",
    "                   'tp': ['t100', 't150', 't250', 't300', 't400', 't500', 'u50', 'r50', 'r100',\n",
    "                          'r150', 'r200', 'r850', 'r925', 'r1000', 'msl', 'tp']  # >0.8\n",
    "                   }\n",
    "# 把字典的所有值合并成一个没有重复元素的集合\n",
    "all_features = list(set(sum(target_features.values(), [])))\n",
    "target = ['t2m', 'u10', 'v10', 'msl', 'tp']\n",
    "for i in target:\n",
    "    all_features.remove(i)\n",
    "    all_features.append(i)\n",
    "\n",
    "data = torch.load('../../data/2011.pt')\n",
    "\n",
    "\n",
    "\n",
    "class ERA5(Dataset):\n",
    "    def __init__(self, data: torch.tensor, features_name: dict) -> None:\n",
    "        self.data = data\n",
    "        self.features = features_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - 1 - 20 + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx].repeat(20, 1, 1, 1)\n",
    "        y = self.data[idx+1:idx+21, -5:]\n",
    "        # x = torch.from_numpy(self.data[idx].values).float().repeat(20, 1, 1, 1)\n",
    "        # y = torch.from_numpy(self.data[idx+1:idx+21, -5:].values).float()\n",
    "\n",
    "        lead_time = torch.arange(1, 21).float()\n",
    "\n",
    "        return x, y, lead_time\n",
    "\n",
    "\n",
    "data_train, data_val = data[:-200], data[-200:]\n",
    "print(data_train.shape, data_train.dtype)\n",
    "print(data_val.shape, data_val.dtype)\n",
    "data_train, data_val = ERA5(data_train, target_features), ERA5(data_val, target_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T04:05:06.845237400Z",
     "start_time": "2023-09-05T04:05:05.527998900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15]],\n",
      "\n",
      "        [[16, 17],\n",
      "         [18, 19]]])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 2, 2])\n",
      "torch.Size([1, 2, 2])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "x = torch.arange(20).view(5,2,2)\n",
    "y = torch.arange(20, 40).view(5,2,2)\n",
    "z = torch.arange(1,21)\n",
    "print(z.shape)\n",
    "print(x)\n",
    "# 每次取一个batch，batch_size=2\n",
    "for x_sub, y_sub, z_sub in DataLoader(list(zip(x, y, z)), batch_size=2):\n",
    "    # print(x_sub)\n",
    "    print(x_sub.shape)\n",
    "    print(y_sub.shape)\n",
    "    print(z_sub.shape)\n",
    "    # print(x_sub)\n",
    "    # print(y_sub)\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T07:23:08.881888200Z",
     "start_time": "2023-09-05T07:23:08.877753800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "def pretrain_one_epoch(epoch, start_step, model, criterion, data_loader, optimizer, loss_scaler, lr_scheduler, min_loss, all_features):\n",
    "    loss_val = torch.tensor(0., device=\"cuda\")\n",
    "    count = torch.tensor(1e-5, device=\"cuda\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        if step < start_step:\n",
    "            continue\n",
    "\n",
    "        x, y, lead_time = [x.cuda() for x in batch]\n",
    "        x, y = x.view(-1, *x.shape[2:]), y.view(-1, *y.shape[2:])\n",
    "        lead_time = lead_time.view(-1)\n",
    "\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x, lead_time, all_features)\n",
    "            loss = criterion(out, y)\n",
    "            if torch.isnan(loss).int().sum() == 0:\n",
    "                count += 1\n",
    "                loss_val += loss\n",
    "\n",
    "        loss_scaler.scale(loss).backward()\n",
    "        loss_scaler.step(optimizer)\n",
    "        loss_scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        if loss.item() < min_loss:\n",
    "            save_model(model, epoch, step+1, optimizer, lr_scheduler, loss_scaler, min_loss, SAVE_PATH/'backbone_batch_latest.pt')\n",
    "\n",
    "    return loss_val.item() / count.item()\n",
    "\n",
    "\n",
    "def train(args, default_vars):\n",
    "    # model = Model(features_name=target_features).cuda()\n",
    "    model = RegionalClimaX(default_vars, img_size=[160,160], patch_size=4, embed_dim=1024, depth=8,\n",
    "                 decoder_depth=2, num_heads=16, mlp_ratio=4, drop_path=0.1, drop_rate=0.1).cuda()\n",
    "    all_features = default_vars\n",
    "    param_sum, buffer_sum, all_size = getModelSize(model)\n",
    "    print(f\"Number of Parameters: {param_sum}, Number of Buffers: {buffer_sum}, Size of Model: {all_size:.4f} MB\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), args.lr, weight_decay=args.weight_decay, betas=(0.9, 0.95))\n",
    "    loss_scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # load data\n",
    "    train_dataloader = DataLoader(data_train, args.batch_size, num_workers=0, pin_memory=True, drop_last=False, shuffle=True)  # , num_workers=8, pin_memory=True\n",
    "    val_dataloader = DataLoader(data_val, args.batch_size, num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # load\n",
    "    start_epoch, start_step, min_loss = load_model(model, optimizer, lr_scheduler, loss_scaler, SAVE_PATH / ' model.pt')\n",
    "    # print(torch.load(SAVE_PATH / 'backbone.pt').items())\n",
    "    # model.load_state_dict(torch.load(SAVE_PATH / 'backbone.pt')['model'])\n",
    "    # start_epoch, start_step = 1, 1\n",
    "    # min_loss = np.inf\n",
    "    print(f\"Start pretrain for {args.pretrain_epochs} epochs\")\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, args.pretrain_epochs)):\n",
    "        t0 = time.time()\n",
    "        train_loss = pretrain_one_epoch(epoch, start_step, model, criterion, train_dataloader, optimizer, loss_scaler,\n",
    "                                        lr_scheduler, min_loss, all_features)\n",
    "        t1 = time.time()\n",
    "        start_step = 0\n",
    "        lr_scheduler.step(epoch)\n",
    "\n",
    "        val_loss = climax_pretrain_evaluate(val_dataloader, model, criterion, all_features)\n",
    "\n",
    "        # if rank == 0 and local_rank == 0:\n",
    "        print(f\"Epoch {epoch} | Train loss: {train_loss:.6f}, Val loss: {val_loss:.6f}, Time: {t1-t0:.2f}s\")\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            save_model(model, path=SAVE_PATH / 'backbone.pt', only_model=True)\n",
    "            save_model(model, epoch + 1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, SAVE_PATH / 'backbone_latest.pt')\n",
    "        save_model(model, epoch + 1, 0, optimizer, lr_scheduler, loss_scaler, min_loss, SAVE_PATH / 'individual_model.pt')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T04:05:06.848771300Z",
     "start_time": "2023-09-05T04:05:06.846248700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 110069360, Number of Buffers: 0, Size of Model: 419.8813 MB\n",
      "Start pretrain for 10 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [02:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 19.04 GiB (GPU 0; 8.00 GiB total capacity; 20.24 GiB already allocated; 0 bytes free; 20.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m args\n\u001B[0;32m     25\u001B[0m args \u001B[38;5;241m=\u001B[39m get_args()\n\u001B[1;32m---> 26\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_features\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[4], line 59\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(args, default_vars)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(start_epoch, args\u001B[38;5;241m.\u001B[39mpretrain_epochs)):\n\u001B[0;32m     58\u001B[0m     t0 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 59\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mpretrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_scaler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     t1 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     62\u001B[0m     start_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "Cell \u001B[1;32mIn[4], line 17\u001B[0m, in \u001B[0;36mpretrain_one_epoch\u001B[1;34m(epoch, start_step, model, criterion, data_loader, optimizer, loss_scaler, lr_scheduler, min_loss, all_features)\u001B[0m\n\u001B[0;32m     13\u001B[0m lead_time \u001B[38;5;241m=\u001B[39m lead_time\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mamp\u001B[38;5;241m.\u001B[39mautocast():\n\u001B[1;32m---> 17\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlead_time\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(out, y)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(loss)\u001B[38;5;241m.\u001B[39mint()\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mE:\\weather\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mE:\\weather_code\\climax\\regional_forecast\\arch.py:63\u001B[0m, in \u001B[0;36mRegionalClimaX.forward\u001B[1;34m(self, x, lead_times, variables)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Forward pass through the model.\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m    preds (torch.Tensor): `[B, Vo, H, W]` shape. Predicted weather/climate variables.\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     62\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39minterpolate(x, size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m160\u001B[39m, \u001B[38;5;241m160\u001B[39m), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 63\u001B[0m out_transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_encoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlead_times\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariables\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# B, L, D\u001B[39;00m\n\u001B[0;32m     64\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(out_transformers)  \u001B[38;5;66;03m# B, L, V*p*p\u001B[39;00m\n\u001B[0;32m     66\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munpatchify(preds)[:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m5\u001B[39m:]\n",
      "File \u001B[1;32mE:\\weather_code\\climax\\regional_forecast\\arch.py:28\u001B[0m, in \u001B[0;36mRegionalClimaX.forward_encoder\u001B[1;34m(self, x, lead_times, variables)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# add variable embedding\u001B[39;00m\n\u001B[0;32m     27\u001B[0m var_embed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_var_emb(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvar_embed, variables)\n\u001B[1;32m---> 28\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mvar_embed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# B, V, L, D\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# variable aggregation\u001B[39;00m\n\u001B[0;32m     31\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate_variables(x)  \u001B[38;5;66;03m# B, L, D\u001B[39;00m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 19.04 GiB (GPU 0; 8.00 GiB total capacity; 20.24 GiB already allocated; 0 bytes free; 20.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "class AttrDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        raise AttributeError(f\"'AttrDict' object has no attribute '{name}'\")\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "def get_args():\n",
    "    # default_vars, img_size=..., patch_size=2, embed_dim=1024, depth=8, decoder_depth=2, \\\n",
    "    #     num_heads=16, mlp_ratio=4, drop_path=0.1, drop_rate=0.1\n",
    "\n",
    "    args = dict(batch_size=4, pretrain_epochs=10, fintune_epochs=25, drop=0.0, drop_path=0.1,\n",
    "                opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1, momentum=0.9, weight_decay=0.05, sched='cosine',\n",
    "                lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05,\n",
    "                decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1,\n",
    "                color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic',\n",
    "                repeated_aug=False, reprob=0, remode='pixel', recount=1, resplit=False, fno_bias=False, fno_blocks=4,\n",
    "                fno_softshrink=0.0, double_skip=False, tensorboard_dir=None, hidden_size=256, num_layers=12,\n",
    "                checkpoint_activations=False, autoresume=False, num_attention_heads=1, ls_w=4, ls_dp_rank=16)\n",
    "    args = AttrDict(args)\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "train(args, all_features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T04:07:37.658013900Z",
     "start_time": "2023-09-05T04:05:06.850813300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save 000.pt ...\n",
      "save 001.pt ...\n",
      "save 002.pt ...\n",
      "save 003.pt ...\n",
      "save 004.pt ...\n",
      "save 005.pt ...\n",
      "save 006.pt ...\n",
      "save 007.pt ...\n",
      "save 008.pt ...\n",
      "save 009.pt ...\n",
      "save 010.pt ...\n",
      "save 011.pt ...\n",
      "save 012.pt ...\n",
      "save 013.pt ...\n",
      "save 014.pt ...\n",
      "save 015.pt ...\n",
      "save 016.pt ...\n",
      "save 017.pt ...\n",
      "save 018.pt ...\n",
      "save 019.pt ...\n",
      "save 020.pt ...\n",
      "save 021.pt ...\n",
      "save 022.pt ...\n",
      "save 023.pt ...\n",
      "save 024.pt ...\n",
      "save 025.pt ...\n",
      "save 026.pt ...\n",
      "save 027.pt ...\n",
      "save 028.pt ...\n",
      "save 029.pt ...\n",
      "save 030.pt ...\n",
      "save 031.pt ...\n",
      "save 032.pt ...\n",
      "save 033.pt ...\n",
      "save 034.pt ...\n",
      "save 035.pt ...\n",
      "save 036.pt ...\n",
      "save 037.pt ...\n",
      "save 038.pt ...\n",
      "save 039.pt ...\n",
      "save 040.pt ...\n",
      "save 041.pt ...\n",
      "save 042.pt ...\n",
      "save 043.pt ...\n",
      "save 044.pt ...\n",
      "save 045.pt ...\n",
      "save 046.pt ...\n",
      "save 047.pt ...\n",
      "save 048.pt ...\n",
      "save 049.pt ...\n",
      "save 050.pt ...\n",
      "save 051.pt ...\n",
      "save 052.pt ...\n",
      "save 053.pt ...\n",
      "save 054.pt ...\n",
      "save 055.pt ...\n",
      "save 056.pt ...\n",
      "save 057.pt ...\n",
      "save 058.pt ...\n",
      "save 059.pt ...\n",
      "save 060.pt ...\n",
      "save 061.pt ...\n",
      "save 062.pt ...\n",
      "save 063.pt ...\n",
      "save 064.pt ...\n",
      "save 065.pt ...\n",
      "save 066.pt ...\n",
      "save 067.pt ...\n",
      "save 068.pt ...\n",
      "save 069.pt ...\n",
      "save 070.pt ...\n",
      "save 071.pt ...\n",
      "save 072.pt ...\n",
      "save 073.pt ...\n",
      "save 074.pt ...\n",
      "save 075.pt ...\n",
      "save 076.pt ...\n",
      "save 077.pt ...\n",
      "save 078.pt ...\n",
      "save 079.pt ...\n",
      "save 080.pt ...\n",
      "save 081.pt ...\n",
      "save 082.pt ...\n",
      "save 083.pt ...\n",
      "save 084.pt ...\n",
      "save 085.pt ...\n",
      "save 086.pt ...\n",
      "save 087.pt ...\n",
      "save 088.pt ...\n",
      "save 089.pt ...\n",
      "save 090.pt ...\n",
      "save 091.pt ...\n",
      "save 092.pt ...\n",
      "save 093.pt ...\n",
      "save 094.pt ...\n",
      "save 095.pt ...\n",
      "save 096.pt ...\n",
      "save 097.pt ...\n",
      "save 098.pt ...\n",
      "save 099.pt ...\n",
      "save 100.pt ...\n",
      "save 101.pt ...\n",
      "save 102.pt ...\n",
      "save 103.pt ...\n",
      "save 104.pt ...\n",
      "save 105.pt ...\n",
      "save 106.pt ...\n",
      "save 107.pt ...\n",
      "save 108.pt ...\n",
      "save 109.pt ...\n",
      "save 110.pt ...\n",
      "save 111.pt ...\n",
      "save 112.pt ...\n",
      "save 113.pt ...\n",
      "save 114.pt ...\n",
      "save 115.pt ...\n",
      "save 116.pt ...\n",
      "save 117.pt ...\n",
      "save 118.pt ...\n",
      "save 119.pt ...\n",
      "save 120.pt ...\n",
      "save 121.pt ...\n",
      "save 122.pt ...\n",
      "save 123.pt ...\n",
      "save 124.pt ...\n",
      "save 125.pt ...\n",
      "save 126.pt ...\n",
      "save 127.pt ...\n",
      "save 128.pt ...\n",
      "save 129.pt ...\n",
      "save 130.pt ...\n",
      "save 131.pt ...\n",
      "save 132.pt ...\n",
      "save 133.pt ...\n",
      "save 134.pt ...\n",
      "save 135.pt ...\n",
      "save 136.pt ...\n",
      "save 137.pt ...\n",
      "save 138.pt ...\n",
      "save 139.pt ...\n",
      "save 140.pt ...\n",
      "save 141.pt ...\n",
      "save 142.pt ...\n",
      "save 143.pt ...\n",
      "save 144.pt ...\n",
      "save 145.pt ...\n",
      "save 146.pt ...\n",
      "save 147.pt ...\n",
      "save 148.pt ...\n",
      "save 149.pt ...\n",
      "save 150.pt ...\n",
      "save 151.pt ...\n",
      "save 152.pt ...\n",
      "save 153.pt ...\n",
      "save 154.pt ...\n",
      "save 155.pt ...\n",
      "save 156.pt ...\n",
      "save 157.pt ...\n",
      "save 158.pt ...\n",
      "save 159.pt ...\n",
      "save 160.pt ...\n",
      "save 161.pt ...\n",
      "save 162.pt ...\n",
      "save 163.pt ...\n",
      "save 164.pt ...\n",
      "save 165.pt ...\n",
      "save 166.pt ...\n",
      "save 167.pt ...\n",
      "save 168.pt ...\n",
      "save 169.pt ...\n",
      "save 170.pt ...\n",
      "save 171.pt ...\n",
      "save 172.pt ...\n",
      "save 173.pt ...\n",
      "save 174.pt ...\n",
      "save 175.pt ...\n",
      "save 176.pt ...\n",
      "save 177.pt ...\n",
      "save 178.pt ...\n",
      "save 179.pt ...\n",
      "save 180.pt ...\n",
      "save 181.pt ...\n",
      "save 182.pt ...\n",
      "save 183.pt ...\n",
      "save 184.pt ...\n",
      "save 185.pt ...\n",
      "save 186.pt ...\n",
      "save 187.pt ...\n",
      "save 188.pt ...\n",
      "save 189.pt ...\n",
      "save 190.pt ...\n",
      "save 191.pt ...\n",
      "save 192.pt ...\n",
      "save 193.pt ...\n",
      "save 194.pt ...\n",
      "save 195.pt ...\n",
      "save 196.pt ...\n",
      "save 197.pt ...\n",
      "save 198.pt ...\n",
      "save 199.pt ...\n",
      "save 200.pt ...\n",
      "save 201.pt ...\n",
      "save 202.pt ...\n",
      "save 203.pt ...\n",
      "save 204.pt ...\n",
      "save 205.pt ...\n",
      "save 206.pt ...\n",
      "save 207.pt ...\n",
      "save 208.pt ...\n",
      "save 209.pt ...\n",
      "save 210.pt ...\n",
      "save 211.pt ...\n",
      "save 212.pt ...\n",
      "save 213.pt ...\n",
      "save 214.pt ...\n",
      "save 215.pt ...\n",
      "save 216.pt ...\n",
      "save 217.pt ...\n",
      "save 218.pt ...\n",
      "save 219.pt ...\n",
      "save 220.pt ...\n",
      "save 221.pt ...\n",
      "save 222.pt ...\n",
      "save 223.pt ...\n",
      "save 224.pt ...\n",
      "save 225.pt ...\n",
      "save 226.pt ...\n",
      "save 227.pt ...\n",
      "save 228.pt ...\n",
      "save 229.pt ...\n",
      "save 230.pt ...\n",
      "save 231.pt ...\n",
      "save 232.pt ...\n",
      "save 233.pt ...\n",
      "save 234.pt ...\n",
      "save 235.pt ...\n",
      "save 236.pt ...\n",
      "save 237.pt ...\n",
      "save 238.pt ...\n",
      "save 239.pt ...\n",
      "save 240.pt ...\n",
      "save 241.pt ...\n",
      "save 242.pt ...\n",
      "save 243.pt ...\n",
      "save 244.pt ...\n",
      "save 245.pt ...\n",
      "save 246.pt ...\n",
      "save 247.pt ...\n",
      "save 248.pt ...\n",
      "save 249.pt ...\n",
      "save 250.pt ...\n",
      "save 251.pt ...\n",
      "save 252.pt ...\n",
      "save 253.pt ...\n",
      "save 254.pt ...\n",
      "save 255.pt ...\n",
      "save 256.pt ...\n",
      "save 257.pt ...\n",
      "save 258.pt ...\n",
      "save 259.pt ...\n",
      "save 260.pt ...\n",
      "save 261.pt ...\n",
      "save 262.pt ...\n",
      "save 263.pt ...\n",
      "save 264.pt ...\n",
      "save 265.pt ...\n",
      "save 266.pt ...\n",
      "save 267.pt ...\n",
      "save 268.pt ...\n",
      "save 269.pt ...\n",
      "save 270.pt ...\n",
      "save 271.pt ...\n",
      "save 272.pt ...\n",
      "save 273.pt ...\n",
      "save 274.pt ...\n",
      "save 275.pt ...\n",
      "save 276.pt ...\n",
      "save 277.pt ...\n",
      "save 278.pt ...\n",
      "save 279.pt ...\n",
      "save 280.pt ...\n",
      "save 281.pt ...\n",
      "save 282.pt ...\n",
      "save 283.pt ...\n",
      "save 284.pt ...\n",
      "save 285.pt ...\n",
      "save 286.pt ...\n",
      "save 287.pt ...\n",
      "save 288.pt ...\n",
      "save 289.pt ...\n",
      "save 290.pt ...\n",
      "save 291.pt ...\n",
      "save 292.pt ...\n",
      "save 293.pt ...\n",
      "save 294.pt ...\n",
      "save 295.pt ...\n",
      "save 296.pt ...\n",
      "save 297.pt ...\n",
      "save 298.pt ...\n",
      "save 299.pt ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.tools import getModelSize, load_model, save_model\n",
    "\n",
    "# 把数据一次全部读取，放入dataset和dataloader\n",
    "data = []\n",
    "for idx in range(300):\n",
    "    idx = str(idx).zfill(3)     # 将idx转换成3位数的字符串，在前面补0\n",
    "    data.append(torch.load(f'../../data/weather_round1_test/input/{idx}.pt', map_location='cpu'))\n",
    "# data.append(torch.load(f'../../data/weather_round1_test/input/000.pt', map_location='cpu'))\n",
    "\n",
    "target = ['t2m', 'u10', 'v10', 'msl', 'tp']\n",
    "\n",
    "model = RegionalClimaX(target, img_size=[160,160], patch_size=4, embed_dim=1024, depth=8,\n",
    "                 decoder_depth=2, num_heads=16, mlp_ratio=4, drop_path=0.1, drop_rate=0.1)\n",
    "model.load_state_dict(torch.load('backbone.pt')['model'])\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "# inference\n",
    "model.eval()\n",
    "for i in range(300):\n",
    "    # output = []\n",
    "    idx = str(i).zfill(3)\n",
    "    data_in = data[i].cuda()\n",
    "    data_in = data_in[-1:].repeat(20, 1, 1, 1)\n",
    "    lead_time = torch.arange(1, 21).float().cuda()\n",
    "    output = model(data_in, lead_time, target).detach().cpu().astype(torch.float16)\n",
    "    # for j in range(20):\n",
    "    #     data_out = model(data_in[-1:], torch.tensor([j+1]).float().cuda(), target)\n",
    "    #     output.append(data_out.cpu().detach().numpy())\n",
    "    # output = np.concatenate(output, axis=0)\n",
    "    # output = torch.tensor(output, dtype=torch.float16)\n",
    "    print(output.shape, output.dtype)\n",
    "    torch.save(output, f'./output/{idx}.pt')\n",
    "    print(f'save {idx}.pt ...')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T14:22:07.644244Z",
     "start_time": "2023-09-05T14:15:24.885820200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 70, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "print(data_in.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T04:31:27.304268200Z",
     "start_time": "2023-09-05T04:31:27.297623400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
